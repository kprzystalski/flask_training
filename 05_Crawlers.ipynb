{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling using Request\n",
    "\n",
    "Crawling over a site is easy to make using the requests package. We can use many different HTTP methods as methods of requests as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "requests.get('https://httpbin.org/post', data={'key':'value'})\n",
    "requests.post('https://httpbin.org/post', data={'key':'value'})\n",
    "requests.put('https://httpbin.org/put', data={'key':'value'})\n",
    "requests.delete('https://httpbin.org/delete')\n",
    "requests.head('https://httpbin.org/get')\n",
    "requests.patch('https://httpbin.org/patch', data={'key':'value'})\n",
    "requests.options('https://httpbin.org/get')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method return a response that you can check the status code or the payload. To get the payload as a JSON, you need to do as below. We know how to deal with JSON data, so now you are able to manipulate the JSON data and use it for your purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://httpbin.org/post', data={'key':'value'})\n",
    "response.status_code\n",
    "json_response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. Request data from our REST API application\n",
    "\n",
    "We have developed an REST API application for grants. Use the requests package in a new application to get the data from our app and save it in a separate database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML files\n",
    "\n",
    "Sometime getting information from a REST API application is not possible, because there is no REST API available. In this case we scrape data. Let's take Amazon and scrape Flask books. We use BeautifulSoup package to work on the HTML file we download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "amazon_page = open('amazon_flask.html','r')\n",
    "soup = bs4.BeautifulSoup(amazon_page, features=\"lxml\")\n",
    "\n",
    "titles = soup.findAll('span',{\"class\": \"a-size-base-plus a-color-base a-text-normal\"})\n",
    "\n",
    "for title in titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2. Parse Amazon webpage with BeautifulSoap\n",
    "\n",
    "Using Request and BeautifulSoap find all prices for each subpage for the keyword: Flask development. Find all prices (also the fractions) and print it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3. Save all requested data into a database using SQLAlchemy\n",
    "\n",
    "Extend the previous example by saving the data into a PostgreSQL database and SQLAlchemy. Prepare a model for the items found at Amazon like: title, image url, price, and review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threading in Python\n",
    "\n",
    "Crawling through whole website can take some time. We can easily measure how long it take to download/scrape a website or REST API. We use the concurrency to split the job into tasks that can be done using more just one core of our CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# your function invoked here\n",
    "duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "thread_local = threading.local()\n",
    "\n",
    "if not hasattr(thread_local, \"session\"):\n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session\n",
    "\n",
    "def download_site(url):\n",
    "    pass # your scrapping code goes here\n",
    "\n",
    "def download_all_sites(sites):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        executor.map(download_site, sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4. Convert the Amazon scrapping script \n",
    "\n",
    "Scrape all book titles for pyhton development term on Amazon using the concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
